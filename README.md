# OSMiniProject2

**OSMiniProject2** is an experimental extension of the **xv6 operating system** that uses an **external LLM agent** (running via Ollama on Windows) to provide **real-time scheduling advice** to the kernel.

The high-level idea:

- xv6 periodically logs per-process scheduler stats into a shared text file.
- A Python **agent bridge** (running in WSL) reads those logs and queries an LLM for â€œwhich PID to run nextâ€.
- The agent writes its decision back to a shared advice file.
- xv6 reads the advice and uses it to influence its scheduler.

## Repository Layout

```text
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ agent_bridge.py      # Reads scheduler logs, queries Ollama, writes ADVICE lines
â”‚   â”œâ”€â”€ analyze_results.py   # Offline analysis: parses shared/sched_log.txt and plots CPU/wait/IO
â”‚   â”œâ”€â”€ test_agent.py        # Unit-style tests: log parsing, prompt generation, LLM connectivity
â”‚   â”œâ”€â”€ test_xv6.py          # Sends synthetic SCHED_LOG blocks â†’ checks agent PID choices
â”‚   â””â”€â”€ test_scheduling.py   # Full simulated scheduler that uses agent advice end-to-end
â”‚
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ sched_log.txt        # Scheduler snapshots (produced by the modified xv6 kernel)
â”‚   â””â”€â”€ llm_advice.txt       # Advice lines (written by agent_bridge.py, consumed by xv6)
â”‚
â”œâ”€â”€ xv6/
â”‚   â”œâ”€â”€ Makefile             # Adds llmhelper + test workloads to UPROGS
â”‚   â”œâ”€â”€ kernel/
â”‚   â”‚   â”œâ”€â”€ defs.h           # Prototypes for scheduling-stat helpers + set_llm_advice
â”‚   â”‚   â”œâ”€â”€ proc.h           # Extended struct proc: cpu_ticks, wait_ticks, io_count, recent_cpu
â”‚   â”‚   â”œâ”€â”€ proc.c           # Tick accounting, state logging, scheduler uses LLM advice
â”‚   â”‚   â”œâ”€â”€ sysproc.c        # sys_set_llm_advice, increments io_count via sys_sleep
â”‚   â”‚   â”œâ”€â”€ syscall.c        # Adds SYS_set_llm_advice to syscall dispatch table
â”‚   â”‚   â”œâ”€â”€ syscall.h        # Defines syscall number
â”‚   â”‚   â”œâ”€â”€ trap.c           # Tick-based stat updates + SCHED_LOG interval triggers
â”‚   â”‚   â””â”€â”€ ... (others unchanged)
â”‚   â””â”€â”€ user/
â”‚       â”œâ”€â”€ llmhelper.c      # Reads ADVICE:PID=X from stdin, calls set_llm_advice()
â”‚       â”œâ”€â”€ cpubound.c       # CPU-heavy workload
â”‚       â”œâ”€â”€ iobound.c        # IO-heavy workload
â”‚       â”œâ”€â”€ mixed.c          # Mixed CPU/IO workload
â”‚       â”œâ”€â”€ init.c           # Spawns llmhelper at boot
â”‚       â”œâ”€â”€ user.h           # Declares set_llm_advice()
â”‚       â”œâ”€â”€ usys.pl          # Generates user stub for set_llm_advice()
â”‚       â””â”€â”€ ... (others unchanged)
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## Software Requirements

### Windows 11 (Host)

* [Ollama](https://ollama.com/)
* At least one local model, e.g. `phi3:mini` (or any compatible Ollama model)

### Ubuntu 22.04 (WSL)

* `qemu-system-misc` (for xv6 / QEMU)
* `python3-venv` (for virtual env + Python tooling)
* Python 3.10+ recommended

## ğŸ–¥ï¸ Windows 11 â€“ Ollama Setup

Run these in **PowerShell** (or a similar terminal) on Windows:

```powershell
# 1. Install Ollama via winget
winget install Ollama.Ollama

# 2. Verify installation
ollama --version

# 3. Pull a model (example: phi3:mini)
ollama pull phi3:mini

# 4. Listen on all interfaces so WSL can reach Ollama
setx OLLAMA_HOST "0.0.0.0:11434"

# 5. Restart Ollama to apply OLLAMA_HOST
taskkill /IM ollama.exe /F 2>$null
ollama serve
```

## ğŸ§ Ubuntu 22.04 (WSL) â€“ Project Setup

```bash
# 1. Install dependencies
sudo apt update
sudo apt install -y qemu-system-misc python3-venv

# 2. Clone the repository
git clone https://github.com/Huey-Lewy/OSMiniProject2
cd OSMiniProject2

# 3. Create and activate a virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 4. Install Python dependencies
pip install -r requirements.txt
```

## Run Instructions

You'll need to use **three terminals** (one on Windows, two in WSL). All paths below assume you're in the project root unless noted.

### ğŸ–¥ï¸ Terminal A (Windows 11): Start Ollama

```bash
# Start the Ollama LLM server (already configured to listen on 0.0.0.0:11434)
ollama serve
```

### ğŸ§  Terminal B (Ubuntu WSL): Start the Agent Bridge

```bash
# From the project root

# Clear any existing logs from sched_log.txt and llm_advice.txt
: > shared/sched_log.txt
: > shared/llm_advice.txt

# Make sure the shared dir and FIFO exist at the ROOT level
mkdir -p shared
[ -p shared/llm_advice.fifo ] || mkfifo shared/llm_advice.fifo

# Start the agent from the agent/ directory
cd agent
python3 agent_bridge.py
```

The agent will:

* Tail `shared/sched_log.txt` for new `SCHED_LOG_START` / `SCHED_LOG_END` blocks.
* For each snapshot, call Ollama with a strict â€œpick one PIDâ€ prompt.
* Write decisions as `ADVICE:PID=<n> TS=<ts> V=1` lines into `shared/llm_advice.txt`.

### ğŸ§© Terminal C (Ubuntu WSL): Build and Run xv6

```bash
# From the project root
cd xv6

# (Re)build kernel + filesystem image
make clean
make fs.img kernel/kernel CPUS=1

# Run QEMU with the input/output pipeline:
#   console_mux â†’ QEMU stdin
#   QEMU stdout/stderr â†’ sched_log_splitter
python3 ../agent/console_mux.py ../shared/llm_advice.fifo \
  | qemu-system-riscv64 \
      -machine virt \
      -bios none \
      -kernel kernel/kernel \
      -m 256M \
      -smp 1 \
      -nographic \
      -global virtio-mmio.force-legacy=false \
      -drive file=fs.img,if=none,format=raw,id=x0 \
      -device virtio-blk-device,drive=x0,bus=virtio-mmio-bus.0 \
  | python3 ../agent/sched_log_splitter.py
```

At runtime:

* The modified xv6 kernel periodically logs scheduler snapshots into `shared/sched_log.txt`.
* The user-space helper `llmhelper` reads `shared/llm_advice.txt` and calls the `set_llm_advice()` syscall.
* The xv6 scheduler reads the current advice and uses it to influence which process to run next.

## System Flow

```text
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Windows 11 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                                                      â”‚
          â”‚    Ollama server                                                     â”‚
          â”‚    (phi3:mini, HTTP API)                                             â”‚
          â”‚                                                                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†‘â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ HTTP (LLM calls)
                                  â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                        WSL / Ubuntu                    â”‚
       â”‚                                                        â”‚
       â”‚  agent_bridge.py                                       â”‚
       â”‚    â”œâ”€ tails shared/sched_log.txt  (scheduler logs)     â”‚
       â”‚    â”œâ”€ calls Ollama over HTTP                           â”‚
       â”‚    â”œâ”€ writes ADVICE:PID=... to:                        â”‚
       â”‚    â”‚     â€¢ shared/llm_advice.txt  (log)                â”‚
       â”‚    â”‚     â€¢ shared/llm_advice.fifo  (live pipe)         â”‚
       â”‚                                                        â”‚
       â”‚  console_mux.py                                        â”‚
       â”‚    â”œâ”€ reads:                                           â”‚
       â”‚    â”‚     â€¢ your keyboard (stdin)                       â”‚
       â”‚    â”‚     â€¢ shared/llm_advice.fifo                      â”‚
       â”‚    â””â”€ merges both â†’ QEMU stdin                         â”‚
       â”‚                                                        â”‚
       â”‚  qemu-system-riscv64 (xv6)                             â”‚
       â”‚    â””â”€ stdout/stderr â†’ sched_log_splitter.py            â”‚
       â”‚                                                        â”‚
       â”‚  sched_log_splitter.py                                 â”‚
       â”‚    â”œâ”€ reads QEMU output                                â”‚
       â”‚    â”œâ”€ strips SCHED_LOG_* blocks â†’ shared/sched_log.txt â”‚
       â”‚    â””â”€ forwards everything else â†’ your terminal         â”‚
       â”‚                                                        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†“â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â”‚ console (stdin/stdout) over QEMU
                                  â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚                 xv6 kernel                  â”‚
                  â”‚                                             â”‚
                  â”‚  init (input router)                        â”‚
                  â”‚    â”œâ”€ only process that reads /dev/console  â”‚
                  â”‚    â”œâ”€ echoes what you type back to console  â”‚
                  â”‚    â”œâ”€ if line starts with "ADVICE:PID=":    â”‚
                  â”‚    â”‚     â†’ send to llmhelper via pipe       â”‚
                  â”‚    â””â”€ else:                                 â”‚
                  â”‚          â†’ send to sh via pipe              â”‚
                  â”‚                                             â”‚
                  â”‚  sh (shell)                                 â”‚
                  â”‚    â””â”€ reads commands from its pipe          â”‚
                  â”‚       (still â€œfeelsâ€ interactive to you)    â”‚
                  â”‚                                             â”‚
                  â”‚  llmhelper                                  â”‚
                  â”‚    â””â”€ reads ADVICE:PID=... lines from pipe  â”‚
                  â”‚       â†’ calls set_llm_advice(pid) syscall   â”‚
                  â”‚                                             â”‚
                  â”‚  scheduler                                  â”‚
                  â”‚    â”œâ”€ logs SCHED_LOG_* snapshots            â”‚
                  â”‚    â”œâ”€ consults latest LLM advice            â”‚
                  â”‚    â””â”€ biases RUNNABLE selection accordingly â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Offline Analysis

After running experiments, you can generate basic plots from the captured scheduler logs:

```bash
# From the project root
cd agent
python3 analyze_results.py
```

This reads `shared/sched_log.txt` and produces a PNG summary (CPU ticks, wait ticks, and I/O counts over time) for each PID, saved in the `shared/` project directory.
